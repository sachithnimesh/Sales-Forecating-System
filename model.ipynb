{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1222f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nhits_pipeline.py\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NHiTSModel\n",
    "from darts.metrics import mae, mape, rmse\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.dataprocessing.transformers import Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8546a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- User settings ---\n",
    "PRODUCTS_DIR = \"products_csv\"         # folder with product CSVs\n",
    "OUTPUT_DIR = \"nhits_models\"           # folder to save per-product models\n",
    "COMMON_MODEL_PATH = \"common_nhits_model.h5\"  # final shared model save path\n",
    "N_TRIALS = 20                         # Optuna trials per product (as requested)\n",
    "HORIZON = 6                           # forecast horizon (months) to optimize for\n",
    "MAX_EPOCHS = 80                       # training epochs (you can reduce for speed)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def safe_name(s):\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', s).strip('_')\n",
    "\n",
    "def load_product_series(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Expect columns: date, product, quantity\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    # set monthly frequency (if monthly data)\n",
    "    ts = TimeSeries.from_dataframe(df, 'date', 'quantity')\n",
    "    return ts, df['product'].iloc[0]  # series, product_name\n",
    "\n",
    "# --- Optuna objective for a single series ---\n",
    "def make_optuna_objective(train_ts, val_ts, input_chunk_hint=24, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Returns an objective(trial) that builds a NHiTS model with trial hyperparams,\n",
    "    fits it and returns validation MAE.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        # sample hyperparameters\n",
    "        # Adjusted range for input_chunk_len to ensure min value is >= 24 if possible\n",
    "        min_icl = max(6, min(24, input_chunk_hint // 2))\n",
    "        max_icl = max(24, input_chunk_hint)\n",
    "        input_chunk_len = trial.suggest_int(\"input_chunk_len\", min_icl, max_icl)\n",
    "        num_blocks = trial.suggest_int(\"num_blocks\", 1, 4)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "        layer_size = trial.suggest_int(\"layer_size\", 32, 512, log=True)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "        model = NHiTSModel(\n",
    "            input_chunk_length = input_chunk_len,\n",
    "            output_chunk_length = horizon,\n",
    "            n_epochs = 20,                          # quick training inside optuna trials\n",
    "            batch_size = batch_size,\n",
    "            num_blocks = num_blocks,\n",
    "            num_layers = num_layers,\n",
    "            layer_widths = [layer_size],            # NHiTS expects widths as list\n",
    "            dropout = dropout,\n",
    "            optimizer_kwargs = {\"lr\": lr},\n",
    "            random_state = 42,\n",
    "            likelihood = None,\n",
    "            pl_trainer_kwargs = {\"accelerator\": \"gpu\"} if DEVICE.startswith(\"cuda\") else {}\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            model.fit(train_ts, verbose=False)\n",
    "            # Forecast on validation set (one-shot)\n",
    "            pred = model.predict(n=horizon)\n",
    "            # Align index length if necessary: compute metrics using last horizon points\n",
    "            # We'll compute MAE between val_ts[:horizon] and pred\n",
    "            # If val_ts length < horizon, use full length\n",
    "            eval_len = min(len(val_ts), len(pred))\n",
    "            val_sub = val_ts[-eval_len:]\n",
    "            pred_sub = pred[:eval_len]\n",
    "            score = mae(val_sub, pred_sub)\n",
    "        except Exception as e:\n",
    "            # If training fails, return a large loss\n",
    "            print(\"Trial failed:\", e)\n",
    "            score = 1e9\n",
    "        # free GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return float(score)\n",
    "    return objective\n",
    "\n",
    "# --- Per-product tuning / training function ---\n",
    "def tune_and_train_product(ts: TimeSeries, product_name: str):\n",
    "    # Train/validation split: last HORIZON months for validation\n",
    "    if len(ts) <= HORIZON + 6:\n",
    "        # very short - use 80/20 split in time\n",
    "        split = int(len(ts) * 0.8)\n",
    "        train_ts, val_ts = ts[:split], ts[split:]\n",
    "    else:\n",
    "        train_ts, val_ts = ts[:-HORIZON], ts[-HORIZON:]\n",
    "\n",
    "    # Scale series\n",
    "    scaler = Scaler()\n",
    "    train_scaled = scaler.fit_transform(train_ts)\n",
    "    val_scaled = scaler.transform(val_ts)\n",
    "\n",
    "    # Make Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    # input_chunk_hint for Optuna is now better constrained\n",
    "    input_chunk_hint = min(36, max(12, len(train_ts)//2))\n",
    "    objective = make_optuna_objective(train_scaled, val_scaled, input_chunk_hint=input_chunk_hint)\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best params for {product_name}: {study.best_params}, best_val_mae={study.best_value}\")\n",
    "\n",
    "    # Build final model with best params and longer epochs on full train+val\n",
    "    best = study.best_params\n",
    "    final_model = NHiTSModel(\n",
    "        input_chunk_length = best.get(\"input_chunk_len\", 24),\n",
    "        output_chunk_length = HORIZON,\n",
    "        n_epochs = MAX_EPOCHS,\n",
    "        batch_size = best.get(\"batch_size\", 32),\n",
    "        num_blocks = best.get(\"num_blocks\", 2),\n",
    "        num_layers = best.get(\"num_layers\", 2),\n",
    "        layer_widths = [best.get(\"layer_size\", 128)],\n",
    "        dropout = best.get(\"dropout\", 0.0),\n",
    "        optimizer_kwargs = {\"lr\": best.get(\"lr\", 1e-3)},\n",
    "        random_state = 42,\n",
    "        pl_trainer_kwargs = {\"accelerator\": \"gpu\"} if DEVICE.startswith(\"cuda\") else {}\n",
    "    )\n",
    "\n",
    "    # Fit on the entire series (train+val)\n",
    "    scaled_full = scaler.fit_transform(ts)\n",
    "    final_model.fit(scaled_full, verbose=True)\n",
    "\n",
    "    # Save model and scaler\n",
    "    filename = os.path.join(OUTPUT_DIR, safe_name(product_name) + \"_nhits.h5\")\n",
    "    final_model.save(filename)\n",
    "    # Save scaler object for later inverse transform\n",
    "    scaler_path = os.path.join(OUTPUT_DIR, safe_name(product_name) + \"_scaler.pkl\")\n",
    "    pd.to_pickle(scaler, scaler_path)\n",
    "\n",
    "    print(f\"Saved model: {filename} and scaler: {scaler_path}\")\n",
    "    return final_model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e898e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CRITICAL FIX: global_model.predict() returns a list of TimeSeries when trained on a list\n",
    "# We must call it once and iterate through the results\n",
    "print(\"Generating global forecasts...\")\n",
    "all_common_preds = global_model.predict(n=FORECAST_H)\n",
    "# Ensure the number of forecasts matches the number of products\n",
    "if len(all_common_preds) != len(product_names):\n",
    "    print(\"Warning: Number of global forecasts does not match number of products.\")\n",
    "\n",
    "print(\"Generating ensemble forecasts...\")\n",
    "# Use enumerate to get the index, which corresponds to the global forecast index\n",
    "for i, (prod_name, ts_scaled) in enumerate(zip(product_names, product_series_list)):\n",
    "    # per-product model file load\n",
    "    per_product_path = os.path.join(OUTPUT_DIR, safe_name(prod_name) + \"_nhits.h5\")\n",
    "    try:\n",
    "        # Load per-product model\n",
    "        per_model = NHiTSModel.load(per_product_path)\n",
    "        # Get the corresponding common forecast for this product\n",
    "        common_pred = all_common_preds[i]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading per-product model or accessing global forecast for {prod_name}. Skipping ensemble: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Generate per-product forecast\n",
    "    per_pred = per_model.predict(n=FORECAST_H)\n",
    "    \n",
    "    # average forecasts (ensure matching lengths)\n",
    "    # inverse transform using per-product scaler\n",
    "    # load scaler\n",
    "    scaler_path = os.path.join(OUTPUT_DIR, safe_name(prod_name) + \"_scaler.pkl\")\n",
    "    scaler = pd.read_pickle(scaler_path)\n",
    "    \n",
    "    # both per_pred and common_pred are scaled series; inverse transform using scaler\n",
    "    per_inv = scaler.inverse_transform(per_pred)\n",
    "    common_inv = scaler.inverse_transform(common_pred)\n",
    "\n",
    "    # convert to pandas and average\n",
    "    per_df = per_inv.pd_dataframe().rename(columns={per_inv.pd_dataframe().columns[0]:\"per_product\"})\n",
    "    com_df = common_inv.pd_dataframe().rename(columns={common_inv.pd_dataframe().columns[0]:\"common\"})\n",
    "    merged = pd.concat([per_df, com_df], axis=1) # fillna(method='ffill') is not usually needed for forecasts of same length\n",
    "    merged['ensemble'] = merged.mean(axis=1)\n",
    "\n",
    "    outpath = os.path.join(FORECAST_OUTPUT, safe_name(prod_name) + \"_forecast.csv\")\n",
    "    merged.to_csv(outpath, index=True)\n",
    "    print(\"Saved forecast:\", outpath)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
